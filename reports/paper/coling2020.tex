\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[super]{nth}
\usepackage{color,soul}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{textcomp}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}

\colingfinalcopy

\title{Lexicographical Semantic Change Detection with BERT}

\author{Salih Furkan Akkurt \\
  Boğaziçi University \\
  Department of Computer Engineering \\
  34342 Bebek, Istanbul, Turkey \\
  {\tt furkan.akkurt@boun.edu.tr}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  I publicly release the code and references in a repository~\cite{akkurt-2023-58t-app-repo}.
\end{abstract}

\section{Introduction}

Dictionaries are great sources of information related to languages.
They contain a lot of information regarding meanings (senses) of words.
As a language evolves, senses of its words change and dictionaries need to be updated accordingly.
Dictionary writers constantly work on manually detecting semantic changes in current usage.
This is a very time-consuming and expensive process.

In this work, I propose a method to automatically detect semantic changes in words by leveraging the contextualizing power of deep learning-based language models.
I use BERT~\cite{devlin-etal-2019-bert} to contextualize words in two corpora and cluster their contextualized representations to detect semantic changes.
The current proposed method in this paper does not detect if a sense of a word morphed into another sense but detects if a word has gained a new sense or lost one.
This method can be extended into detecting morphing of senses also.

\blfootnote{
    \hspace{-0.65cm}
    This work is licensed under a Creative Commons
    Attribution 4.0 International Licence.
    Licence details: \url{http://creativecommons.org/licenses/by/4.0/}.
}

\section{System Description}

\subsection{Data}

I use 3 types of data in my work.
The first type is a frequency list of words, the second a dictionary and the third corpora.

Firstly, I use the frequency list of words from the Corpus of Contemporary American English (COCA)~\cite{english-corpora-2008-coca}.
This list serves as my vocabulary which I use to detect semantic changes in.
I restrict my vocabulary to nouns only.
There are 2635 nouns in the list.

Secondly, I use Merriam-Webster (MW)~\cite{mw-2023-dictionary} as my dictionary.
Merriam-Webster is a well-known dictionary that is updated quite frequently.
I have used its API~\cite{mw-2023-api} to get the number of senses of the words in my vocabulary.
These counts serve two purposes: (1) to determine the semantic distance of the embeddings that is necessary for a sense distinction in dictionaries and (2) to, in the end, compare the number of senses of the words in the dictionary with the cluster counts of the word embeddings in the corpora.

Lastly, I use free samples~\cite{corpus-data-2023-corpora} of the COCA and the NOW corpus (News on the Web)~\cite{english-corpora-2016-now} as my two corpora that serve as the current usage of the language.
The COCA's sample has 8.9 million words of linear tokenized text and the one of the NOW's has 1.7 million words.
Content of the COCA ranges from academic usage to fiction and spoken language.
The NOW corpus is a collection of web-based news sources.
Since the task is detecting current semantic changes, the corpora should be recent.

\subsection{Method}

Firstly, the two corpora is merged into one.
When a given sentence is passed through BERT, each token of the sentence has its own contextual representation, as demonstrated by~\cite{pasini-etal-2020-clubert}.
I use BERT~\cite{devlin-etal-2019-bert} to contextualize words in the merged corpus and gather token representations of each word.
After this step, each word has its own sets of representations.
\cite{zhou-li-2020-temporalteller} used "the sum of the last 4 layers [of BERT] to encode both word meaning and context information".
Instead of summation, each representation is the concatenation of the last 4 layers of BERT's output in this work.
The representations are stored after a rounding operation to reduce the size of the data as the concatenation increases the size of the data by 4 times as opposed to summation.
Experiments showed that the rounding operation does not affect the results significantly.

After the contextualization step, the agglomerative clustering algorithm~\cite{florek1951liaison} is used to cluster the representations of each word.
The implementation of the algorithm on scikit-learn~\cite{scikit-learn} requires the number of clusters or the distance threshold to be specified.
I use the number of senses of the words in the dictionary as the number of clusters input to the algorithm.
The cosine distance metric is used to calculate the distances between these clusters.
The minimum distance between the clusters is then used as the distance threshold in the second clustering step.
This distance represents the semantic distance that compelled the dictionary writers to distinguish the senses of the word.

The second clustering step is the same with the first one except the distance threshold is used instead of the number of clusters.
This step is used to create new clusters without specifically knowing the number of clusters beforehand but the distance threshold.
In the agglomerative clustering algorithm, the distance threshold is the maximum distance between two clusters to be merged.
In the end, the number of clusters is the number of senses of the word represented in the corpora.

If the number of clusters of the second clustering is greater than the number of senses of the word in the dictionary, it means that the word has gained a new sense.
If the number of clusters is less than the number of senses of the word, it means that the word has lost a sense.
However, if the number of clusters is equal to the number of senses of the word, it does not mean that the word has not changed semantically, as it may have gained a sense and lost another at the same time.

\section{Experimental Setup}

\section{Results and Discussion}

\subsection{Discussion}

\section{Conclusion}

\bibliographystyle{coling}
\bibliography{coling2020}

\end{document}
